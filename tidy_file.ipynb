{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>percentage</th>\n",
       "      <th>ctx-4</th>\n",
       "      <th>ctx-3</th>\n",
       "      <th>ctx-2</th>\n",
       "      <th>ctx-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>290.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.751131</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.504902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>271.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.753968</td>\n",
       "      <td>0.659341</td>\n",
       "      <td>0.645022</td>\n",
       "      <td>0.686508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>149.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.735043</td>\n",
       "      <td>0.491582</td>\n",
       "      <td>0.675214</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.801347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>342.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.898990</td>\n",
       "      <td>0.831909</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.943333</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.513333</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id    year  month   day  hour  percentage     ctx-4     ctx-3  \\\n",
       "0       290.0  2019.0    7.0  22.0   8.0    0.751131  0.352941  0.352941   \n",
       "1       271.0  2022.0    6.0  10.0  21.0    0.769231  0.753968  0.659341   \n",
       "2       149.0  2022.0    6.0   8.0  20.0    0.735043  0.491582  0.675214   \n",
       "3       342.0  2020.0    2.0   4.0   4.0    0.777778  0.898990  0.831909   \n",
       "4       358.0  2021.0    5.0  28.0   8.0    0.943333  0.480000  0.480000   \n",
       "\n",
       "      ctx-2     ctx-1  \n",
       "0  0.352941  0.504902  \n",
       "1  0.645022  0.686508  \n",
       "2  0.864198  0.801347  \n",
       "3  0.777778  0.777778  \n",
       "4  0.513333  0.766667  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "station_dataframe = dd.read_csv('data/data_bicing_joined_HX.csv', assume_missing=True, delimiter=';')\n",
    "\n",
    "weather_dataframe = dd.read_csv('weather_data/weather.csv', assume_missing=True, delimiter=',')\n",
    "\n",
    "station_dataframe = station_dataframe.loc[station_dataframe['status'] == 'IN_SERVICE']\n",
    "\n",
    "bare_df = station_dataframe[['station_id', 'year', 'month', 'day', 'hour', '% Docks Availlable',  '% Docks Available H-4','% Docks Available H-3', '% Docks Available H-2', '% Docks Available H-1']]\n",
    "bare_df = bare_df.rename(columns={'% Docks Availlable': 'percentage'})\n",
    "for i in range(1, 5):\n",
    "    bare_df = bare_df.rename(columns={f'% Docks Available H-{i}': f'ctx-{i}'})\n",
    "\n",
    "# Print the head of the updated DataFrame\n",
    "\n",
    "bare_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148774780"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bare_df.size.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_prep(weather_df:dd) -> dd:\n",
    "    \n",
    "    weather = weather_df.copy()\n",
    "\n",
    "    weather = weather.groupby(weather.index//2).mean()\n",
    "    weather['mm_precip'] = weather['mm_precip']*2\n",
    "    weather['timestamp'] = (weather['timestamp']-900).astype(int)\n",
    "    weather['datetime'] = weather['timestamp'].map(lambda x: pd.to_datetime(x, unit='s'))\n",
    "\n",
    "    \n",
    "    weather['timestamp'] = weather['timestamp'].astype(int)\n",
    "\n",
    "    weather['datetime'] = weather['timestamp'].map(lambda x: pd.to_datetime(x, unit='s'))\n",
    "    \n",
    "    return weather\n",
    "\n",
    "def weather_merge(weather_df:dd, station_data:dd) -> dd:\n",
    "    weather = weather_df.copy()\n",
    "    stations = station_data.copy()\n",
    "\n",
    "    stations[['year', 'month', 'day', 'hour']] = stations[['year', 'month', 'day', 'hour']].astype(int)\n",
    "\n",
    "    stations['datetime'] = dd.to_datetime(stations['year'].astype(str) + '-' +\n",
    "                                                stations['month'].astype(str) + '-' +\n",
    "                                                stations['day'].astype(str) + ' ' +\n",
    "                                                stations['hour'].astype(str) + ':00:00')\n",
    "    stations = stations.merge(weather[['datetime', 'temperature','mm_precip']], on='datetime', how='left')\n",
    "    # for i in range(1,5):\n",
    "    #     df_weather_shifted = weather.copy()\n",
    "    #     df_weather_shifted['datetime'] = df_weather_shifted['datetime'] + pd.Timedelta(hours=-i)\n",
    "    #     stations = stations.merge(df_weather_shifted[['datetime', 'temperature','mm_precip']], on='datetime', how='inner', suffixes=('', f'-{abs(i)}'))\n",
    "\n",
    "    return stations\n",
    "\n",
    "\n",
    "def extra_time_info(df:dd) -> dd:\n",
    "\n",
    "    def is_weekend(day_of_week):\n",
    "        return 1 if day_of_week >= 5 else 0\n",
    "\n",
    "    df['is_weekend'] = df['datetime'].dt.dayofweek.map(is_weekend, meta=('is_weekend', 'int64'))\n",
    "\n",
    "    df['timeframe1'] = df['datetime'].dt.hour.map(lambda x: 1 if x <= 4 else 0, meta=('timeframe1', 'int64'))\n",
    "    df['timeframe2'] = df['datetime'].dt.hour.map(lambda x: 1 if x >= 5 and x <=9 else 0, meta=('timeframe1', 'int64'))\n",
    "    df['timeframe3'] = df['datetime'].dt.hour.map(lambda x: 1 if x >= 10 and x <=14 else 0, meta=('timeframe1', 'int64'))\n",
    "    df['timeframe4'] = df['datetime'].dt.hour.map(lambda x: 1 if x >= 15 and x <=19 else 0, meta=('timeframe1', 'int64'))\n",
    "    df['timeframe5'] = df['datetime'].dt.hour.map(lambda x: 1 if x >= 20 else 0, meta=('timeframe1', 'int64'))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def station_loc(id_lat_lon:dd, df:dd) -> dd:\n",
    "\n",
    "    assert all(item in list(id_lat_lon.columns) for item in ['station_id', 'lat', 'lon']), 'id_lat_lon must contain station_id, lat and lon columns'\n",
    "    id_locator = id_lat_lon.copy()\n",
    "    data = df.copy()\n",
    "    id_locator = id_locator.drop_duplicates(subset=['station_id'])\n",
    "\n",
    "    data = data.merge(id_locator[['station_id', 'lat', 'lon']], on='station_id', how='left')\n",
    "    data = data.drop(['station_id'], axis=1)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_prepped = weather_prep(weather_dataframe)\n",
    "\n",
    "data_prepared = weather_merge(weather_prepped, bare_df)\n",
    "\n",
    "# data_prepared = extra_time_info(bare_df)\n",
    "\n",
    "# data_prepared = data_prepared.drop(['datetime'], axis=1)\n",
    "\n",
    "# locations = station_dataframe[['station_id', 'lat', 'lon']]\n",
    "\n",
    "# data_prepared = station_loc(id_lat_lon=locations, df=data_prepared)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193425466"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prepared.size.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prepared = data_prepared[['year', 'month', 'day', 'hour', 'is_weekend', 'timeframe1',\n",
    "       'timeframe2', 'timeframe3', 'timeframe4', 'timeframe5', 'lat', 'lon', \n",
    "       'temperature-4', 'mm_precip-4', 'ctx-4', 'temperature-3', 'mm_precip-3', 'ctx-3', \n",
    "       'temperature-2', 'mm_precip-2', 'ctx-2', 'temperature-1', 'mm_precip-1', 'ctx-1', \n",
    "       'temperature', 'mm_precip', 'percentage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401881392"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prepared.size.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════╤════════╤═════════╤═══════╤════════╤══════════════╤══════════════╤══════════════╤══════════════╤══════════════╤═════════╤═════════╤═════════════════╤═══════════════╤══════════╤═════════════════╤═══════════════╤══════════╤═════════════════╤═══════════════╤══════════╤═════════════════╤═══════════════╤══════════╤═══════════════╤═════════════╤══════════════╕\n",
      "│    │   year │   month │   day │   hour │   is_weekend │   timeframe1 │   timeframe2 │   timeframe3 │   timeframe4 │     lat │     lon │   temperature-4 │   mm_precip-4 │    ctx-4 │   temperature-3 │   mm_precip-3 │    ctx-3 │   temperature-2 │   mm_precip-2 │    ctx-2 │   temperature-1 │   mm_precip-1 │    ctx-1 │   temperature │   mm_precip │   percentage │\n",
      "╞════╪════════╪═════════╪═══════╪════════╪══════════════╪══════════════╪══════════════╪══════════════╪══════════════╪═════════╪═════════╪═════════════════╪═══════════════╪══════════╪═════════════════╪═══════════════╪══════════╪═════════════════╪═══════════════╪══════════╪═════════════════╪═══════════════╪══════════╪═══════════════╪═════════════╪══════════════╡\n",
      "│  0 │   2019 │       7 │    22 │      8 │            0 │            0 │            0 │            0 │            0 │ 41.4373 │ 2.1741  │           32.2  │             0 │ 0.352941 │           32.35 │             0 │ 0.352941 │           32.35 │             0 │ 0.352941 │            31.7 │             0 │ 0.504902 │         31    │           0 │     0.751131 │\n",
      "├────┼────────┼─────────┼───────┼────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼─────────┼─────────┼─────────────────┼───────────────┼──────────┼─────────────────┼───────────────┼──────────┼─────────────────┼───────────────┼──────────┼─────────────────┼───────────────┼──────────┼───────────────┼─────────────┼──────────────┤\n",
      "│  1 │   2022 │       6 │    10 │     21 │            0 │            0 │            1 │            0 │            0 │ 41.4506 │ 2.19236 │           19.8  │             0 │ 0.753968 │           19.85 │             0 │ 0.659341 │           20.5  │             0 │ 0.645022 │            21   │             0 │ 0.686508 │         21.05 │           0 │     0.769231 │\n",
      "├────┼────────┼─────────┼───────┼────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼─────────┼─────────┼─────────────────┼───────────────┼──────────┼─────────────────┼───────────────┼──────────┼─────────────────┼───────────────┼──────────┼─────────────────┼───────────────┼──────────┼───────────────┼─────────────┼──────────────┤\n",
      "│  2 │   2022 │       6 │     8 │     20 │            0 │            0 │            1 │            0 │            0 │ 41.3959 │ 2.19295 │           20.05 │             0 │ 0.491582 │           20.05 │             0 │ 0.675214 │           20.05 │             0 │ 0.864198 │            20.3 │             0 │ 0.801347 │         20.75 │           0 │     0.735043 │\n",
      "├────┼────────┼─────────┼───────┼────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼─────────┼─────────┼─────────────────┼───────────────┼──────────┼─────────────────┼───────────────┼──────────┼─────────────────┼───────────────┼──────────┼─────────────────┼───────────────┼──────────┼───────────────┼─────────────┼──────────────┤\n",
      "│  3 │   2020 │       2 │     4 │      4 │            0 │            1 │            0 │            0 │            0 │ 41.4035 │ 2.19366 │           15.7  │             0 │ 0.89899  │           14.35 │             0 │ 0.831909 │           14.6  │             0 │ 0.777778 │            15.4 │             0 │ 0.777778 │         16.65 │           0 │     0.777778 │\n",
      "├────┼────────┼─────────┼───────┼────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼─────────┼─────────┼─────────────────┼───────────────┼──────────┼─────────────────┼───────────────┼──────────┼─────────────────┼───────────────┼──────────┼─────────────────┼───────────────┼──────────┼───────────────┼─────────────┼──────────────┤\n",
      "│  4 │   2021 │       5 │    28 │      8 │            0 │            0 │            0 │            0 │            0 │ 41.3872 │ 2.1793  │           24.4  │             0 │ 0.48     │           24.35 │             0 │ 0.48     │           24.65 │             0 │ 0.513333 │            24.1 │             0 │ 0.766667 │         23.95 │           0 │     0.943333 │\n",
      "╘════╧════════╧═════════╧═══════╧════════╧══════════════╧══════════════╧══════════════╧══════════════╧══════════════╧═════════╧═════════╧═════════════════╧═══════════════╧══════════╧═════════════════╧═══════════════╧══════════╧═════════════════╧═══════════════╧══════════╧═════════════════╧═══════════════╧══════════╧═══════════════╧═════════════╧══════════════╛\n"
     ]
    }
   ],
   "source": [
    "print(data_prepared.head().to_markdown(tablefmt = 'fancy_grid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reduced_data_prepared = data_prepared.head(1000)\n",
    "\n",
    "reduced_data_prepared = reduced_data_prepared[reduced_data_prepared['hour'].isin([4,9,14,19,23])]\n",
    "\n",
    "X = reduced_data_prepared.drop(['percentage'], axis=1)\n",
    "y = reduced_data_prepared['percentage']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 729 candidates, totalling 2187 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                    callbacks=None, colsample_bylevel=None,\n",
       "                                    colsample_bynode=None,\n",
       "                                    colsample_bytree=None,\n",
       "                                    early_stopping_rounds=None,\n",
       "                                    enable_categorical=False, eval_metric=None,\n",
       "                                    feature_types=None, gamma=None, gpu_id=None,\n",
       "                                    grow_policy=None, importance_type=None,\n",
       "                                    interaction_constraints=None,\n",
       "                                    learning_rate=None, m...\n",
       "                                    min_child_weight=None, missing=nan,\n",
       "                                    monotone_constraints=None, n_estimators=100,\n",
       "                                    n_jobs=None, num_parallel_tree=None,\n",
       "                                    predictor=None, random_state=69, ...),\n",
       "             param_grid={'learning_rate': [0.1, 0.01, 0.001],\n",
       "                         'max_depth': [3, 5, 7],\n",
       "                         'n_estimators': [100, 200, 300],\n",
       "                         'reg_alpha': [0, 0.1, 1.0],\n",
       "                         'reg_lambda': [0.1, 1.0, 10.0],\n",
       "                         'subsample': [0.8, 0.9, 1.0]},\n",
       "             scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "space = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'reg_lambda': [0.1, 1.0, 10.0],\n",
    "    'reg_alpha': [0, 0.1, 1.0],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', random_state = 69)\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=space, scoring='neg_mean_squared_error', cv=3, verbose=1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "parameter = open('best_params.json', 'w+')\n",
    "parameter.write(json.dumps(grid_search.best_params_))\n",
    "parameter.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(objective='reg:squarederror', n_estimators=1000, max_depth=6, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8, gamma=0.1, reg_alpha=0.1, reg_lambda=0.1, n_jobs=-1, random_state=123)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BILSTM trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ptbad\\anaconda3\\lib\\site-packages\\dask\\array\\core.py:1701: FutureWarning: The `numpy.column_stack` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ptbad\\anaconda3\\lib\\site-packages\\dask\\array\\core.py:1701: FutureWarning: The `numpy.column_stack` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ptbad\\anaconda3\\lib\\site-packages\\dask\\array\\core.py:1701: FutureWarning: The `numpy.column_stack` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ptbad\\anaconda3\\lib\\site-packages\\dask\\array\\core.py:1701: FutureWarning: The `numpy.column_stack` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# Assuming your dataframe is named 'df'\n",
    "# Extract the static data, the target and the time series data in seperate arrays\n",
    "\n",
    "static_data = data_prepared[['year', 'month', 'day', 'hour', 'is_weekend', 'timeframe1','timeframe2', 'timeframe3', 'timeframe4', 'lat', 'lon', 'temperature', 'mm_precip']].values\n",
    "\n",
    "timestep_1 = data_prepared[['temperature-1', 'mm_precip-1', 'ctx-1']].values\n",
    "timestep_2 = data_prepared[['temperature-2', 'mm_precip-2', 'ctx-2']].values\n",
    "timestep_3 = data_prepared[['temperature-3', 'mm_precip-3', 'ctx-3']].values\n",
    "timestep_4 = data_prepared[['temperature-4', 'mm_precip-4', 'ctx-4']].values\n",
    "\n",
    "time_series_data = np.stack((timestep_4, timestep_3, timestep_2, timestep_1), axis=1)\n",
    "\n",
    "target = data_prepared[['percentage']].values\n",
    "# timestep_1 = np.column_stack((data_prepared[['temperature-1', 'mm_precip-1', 'ctx-1']].values, static_data))\n",
    "# timestep_2 = np.column_stack((data_prepared[['temperature-2', 'mm_precip-2', 'ctx-2']].values, static_data))\n",
    "# timestep_3 = np.column_stack((data_prepared[['temperature-3', 'mm_precip-3', 'ctx-3']].values, static_data))\n",
    "# timestep_4 = np.column_stack((data_prepared[['temperature-4', 'mm_precip-4', 'ctx-4']].values, static_data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " ts_input (InputLayer)          [(None, 4, 3)]       0           []                               \n",
      "                                                                                                  \n",
      " static_input (InputLayer)      [(None, 13)]         0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 32)           4608        ['ts_input[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           448         ['static_input[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 64)           0           ['lstm[0][0]',                   \n",
      "                                                                  'dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            65          ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,121\n",
      "Trainable params: 5,121\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import LSTM, Dense, Input, concatenate, Dropout\n",
    "\n",
    "ts_input = tf.keras.Input(shape=(4, 3), name='ts_input')\n",
    "static_input = tf.keras.Input(shape = (13,), name='static_input')\n",
    "LSTMout = LSTM(32, activation='relu', return_sequences=False)(ts_input)\n",
    "dropout_lstm = Dropout(0.2)(LSTMout)\n",
    "static_out = Dense(32, activation='relu')(static_input)\n",
    "dropout_static = Dropout(0.2)(static_out)\n",
    "\n",
    "merged_out = concatenate([LSTMout, static_out])\n",
    "merged_out = Dense(1, activation='relu')(merged_out)\n",
    "\n",
    "model = tf.keras.Model(inputs=[ts_input, static_input], outputs=merged_out)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create the testing dataframe\n",
    "testing = pd.DataFrame(data={'A': [1, 2, 3, 4], 'Name': ['Carlos', 'Jose', 'Maria', 'Juan'],\n",
    "                             'p': [0.1, 0.2, 0.3, 0.4], 'p-1': [0.2, 0.3, 0.4, 0.5],\n",
    "                             'p-2': [0.3, 0.4, 0.5, 0.6], 't-1': [10, 20, 30, 40],\n",
    "                             't-2': [15, 25, 35, 45]})\n",
    "\n",
    "# Extract the variables from the dataframe\n",
    "static_vars = testing[['A', 'Name']].values\n",
    "target_var = testing['p'].values\n",
    "timestep_1 = testing[['p-1', 't-1']].values\n",
    "timestep_2 = testing[['p-2', 't-2']].values\n",
    "\n",
    "# Combine the variables into a numpy array\n",
    "data = np.column_stack((static_vars, timestep_2, timestep_1))\n",
    "\n",
    "# Reshape the data into a 3D tensor\n",
    "timesteps = 3  # Number of previous timesteps\n",
    "features = data.shape[1] - 2  # Number of features excluding static variables\n",
    "num_samples = len(data) - timesteps + 1\n",
    "\n",
    "data_3d = np.zeros((num_samples, timesteps, features))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    data_3d[i] = data[i:i + timesteps, 2:]\n",
    "\n",
    "# Split the data into input sequences (X) and target values (y)\n",
    "X = data_3d\n",
    "y = target_var[timesteps - 1:]\n",
    "\n",
    "# Create the time series dataset\n",
    "dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    X,\n",
    "    y,\n",
    "    sequence_length=timesteps,\n",
    "    batch_size=32  # Specify your desired batch size\n",
    ")\n",
    "\n",
    "# Now, 'dataset' is a TensorFlow Dataset object that you can use for training your LSTM model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 'Carlos', 0.3, 15.0],\n",
       "        [2, 'Jose', 0.4, 25.0],\n",
       "        [3, 'Maria', 0.5, 35.0],\n",
       "        [4, 'Juan', 0.6, 45.0]],\n",
       "\n",
       "       [[1, 'Carlos', 0.2, 10.0],\n",
       "        [2, 'Jose', 0.3, 20.0],\n",
       "        [3, 'Maria', 0.4, 30.0],\n",
       "        [4, 'Juan', 0.5, 40.0]]], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestep_1 = np.column_stack((testing[['A', 'Name']].values, testing[['p-1', 't-1']].values))\n",
    "timestep_2 = np.column_stack((testing[['A', 'Name']].values, testing[['p-2', 't-2']].values))\n",
    "\n",
    "tensor = np.stack((timestep_2, timestep_1), axis=0)\n",
    "\n",
    "tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
